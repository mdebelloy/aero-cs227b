<html>
  <head>
    <title>Aero</title>
    <script type='text/javascript' src='/epilog/javascript/epilog.js'></script>
    <script type='text/javascript' src='../javascript/localstorage.js'></script>
    <script type='text/javascript' src='../reasoning/general.js'></script>
    <script type='text/javascript'>

      //==============================================================================
      // aero.js
      //==============================================================================



      /* WRITEUP:

      We implemented MCTS with UCB, indexing and differential update as seen in class. An explanation of each function is given below. While most
      of this code is just from lecture, we note that we still use an evaluation function that is a linear combination of mobility and intermediate
      reward. After manually playing around with these, we decided to set the mobility coefficient to 0.2, making the linear combination 80% 
      intermediate reward. Similarly, we set the C coefficient in the UCB function to 1.0. We note that the original paper uses sqrt(2) or 
      approximately 1.4, which is provides optimal regret bounds for multi armed bandit problems, but we chose a slightly more conservative value.
      The description of each function is below, and the indexing and differential update parts are taken straigth from the lecture slides.

      CODE SPECIFICATIONS

      Helper functions:
      -----------------
      - For the detailed descriptions of the following helper functions, 
        please refer to the comments provided for the functions below. 

      1. shuffle(arr): Shuffles an array in place. 
      2. numberize(str): Turn an input string to an integer. 
      3. mobility(r, role, library): Computes the percentage of available moves among all moves. 
      4. intermediate_reward(role, state): Compute the reward of a given state. 
      5. evaluate(role, state): Computes the weighted valuation for a state and given role,  
          as a weighted sum of long-term mobility vs. immediate rewards. 
      6. makenode(state, mover, reward): Initializes a new node for the MCTS tree. 


      Primary functions: 
      ------------------ 
      1. play(move): 
        - Runs a single move in the MCTS loop. 
        - If move is given, updates the current tree. 
        - Otherwise, initializes a new tree. 
        - Limits the search based on time limits of the player.
        - Logs best found move every 1000 iterations (chosen as the default value). 
        - Selects and returns the move with best average reward. 

      2. getBestAction(node): 
        - Selects the child with highest utility. 
        - Runs a greedy search among the children of the current node, using MCTS. 
      
      3. process(node, deadline):
        - Runs the recursive calls to the MCTS, covering simulation, tree expansion and
          backpropagation. 
        - If the search arrives at a terminal state, returns the final reward. 
        - For all unexpanded nodes, generates all legal successor states for exploration. 
          - Loops through the children nodes, simulates MCTS for all non-terminal children, 
            assigns the results as utilities to each node. 
          - Backpropagates the assigned utility values from child nodes to the current nodes 
            to determine the best action. 
        - For all expanded nodes, recurse onto the best child and backpropagate the utility values. 

      4. expand(node):
        - Adds all legal children nodes to queue for exploration. 
        - Randomly shuffles legal actions. 
        - For all terminal states, uses evaluate(role, state) function to compute the
          weighted expected average reward of the given state. 
      
      5. select(node):
        - Selects the child node with the highest UCB value. 
        - The function defined for the UCB value is implemented from the same formula 
          provided in the lecture slides. 
      
      6. ucbValue(node, parentVisits): 
        - Computes the UCB formula (implemented from the lecture slides). 
        - Designed to balance long vs. short term rewards. 
        - The hyperparameter C = 1.0 is chosen as the default, but not tested experimentally
          to converge into the optimal value. 
          - Testing and optimizing the value for C remains as an open improvement area.

      7. update(node): 
        - Updates utility values for children.
        - Uses scoremax() for the player, and scoremin() for the opponent. 
          under the objective of maximizing the player's actions and expecting the worst
          possible outcome from the opponent. 

      8. depthcharge(state):
        - Simulates a randomized game from a given state.
        - Recursively plays randomized actions until reaching the terminal state, and 
          returns final rewards. 
        - Performs an unbounded depth search, capped by the time limitations of the player. 
      */

      var manager = 'manager';
      var player = 'aero';

      var role = 'robot';
      var rules = [];
      var startclock = 10;
      var playclock = 10;

      var library = [];
      var roles = [];
      var state = [];

      var tree;
      var C = 1.0; 
      var mobilityCoef = 0.2; 
      var dataindexing = false; 
      var depthcharges = 0; 

      /* shuffle(arr): 
      Description: 
      - Shuffles an input array in place.
      Input: 
      - array: The input array to be shuffled.
      Returns:
      - array: The shuffled array. 
      */ 
      function shuffle(array) {
        for (var i = array.length - 1; i > 0; i--) {
          var j = Math.floor(Math.random() * (i + 1));
          var temp = array[i];
          array[i] = array[j];
          array[j] = temp;
        }
        return array;
      }

      /* numberize(str):
      Description:
      - Takes in an input string, and returns the integer associated
        with the numerical value of the string. 
      Input:
      - str: The input string, in numerals. 
      Returns:
      - parseInt(str): The numerical value of the input string.
      */
      function numberize(str) {
        return parseInt(str);
      }

      /* mobility(r, state, library):
      Description:
        - Determines the mobility of a given player in a state, 
          as a function of the percentage of the available moves to the player.
      Input: 
        - r: Assigned role of the player. 
        - state: The current state.
        - library: The library consisting of the state graph. 
      Returns:
        - If it is the player's turn, return the percentage of legal moves for the player. 
        - Otherwise (opponent's turn), return the percentage of legal moves for the player. 
      */
      function mobility(r, state, library) {
        var actions = findlegals(state, library);
        var feasibles = findactions(library);

        if (r === role) {
          return (actions.length/feasibles.length) * 100;
        } else {
          return 100 - (actions.length/feasibles.length) * 100;
        }
      }

      /* intermediate_reward(role, state):
      Description: 
        - Computes the reward of a given state, for a given player.
      Input: 
        - role: Role of the current player.
        - state: Current state. 
      Returns:
        - The reward of the current state. 
      */
      function intermediate_reward(role, state) {
        return findreward(role, state, library) * 1;
      }

      /* evaluate(role, state): 
      Description:
        - Weights the long-term mobility with immediate short-term gains.
        - Designed to balance the long-term vs. short-term rewards of a given decision. 
      Input: 
        - role: Role of the current player. 
        - state: Current state.
      Returns:
        - The weighted valuation of the current state.
      */
      function evaluate(role, state) {
        return mobilityCoef * mobility(role, state, library) + (1 - mobilityCoef) * intermediate_reward(role, state);
      }

      /* play(move):
      Description:
        - Perform one MCTS play cycle based on the given move. 
        - Continue running MCTS cycles until the time runs out. 
        - Log results every 1000 iterations (chosen by default). 
        - Find the move with best average reward. 
      Input: 
        - move: An action for which an MCTS play cycle is to be performed. 
      Returns:
        - false: If it is not the player's turn.
        - bestMove.action: Otherwise, action with the best average reward.  
      */
      function play(move) {
        if (move !== nil) {
          tree = subtree(move, tree);
          state = tree.state;
        } else {
          tree = makenode(state, findcontrol(state, library), evaluate(role, state));
        }
        
        if (findcontrol(state, library) !== role) {
          return false;
        }
        
        var deadline = Date.now() + (playclock - 2) * 1000;
        var iterations = 0;
        
        while (Date.now() < deadline) {
          process(tree, deadline);
          iterations++;
          
          if (iterations % 1000 === 0) {
            var timeRemaining = Math.max(0, (deadline - Date.now()) / 1000);
            console.log("Time remaining: " + timeRemaining.toFixed(1) + "s, iterations: " + iterations);
            
            var bestAction = getBestAction(tree);
            console.log("Current best action: " + bestAction.action + 
                        ", visits: " + bestAction.visits + 
                        ", score: " + bestAction.score);
          }
        }
        
        var bestMove = getBestAction(tree);
        console.log("Final decision:");
        console.log("- Action: " + bestMove.action);
        console.log("- Score: " + bestMove.score);
        console.log("- Visits: " + bestMove.visits);
        
        var newState = simulate(bestMove.action, tree.state, library);
        var terminal = findterminalp(newState, library);
        if (terminal) {
          var actualReward = findreward(role, newState, library) * 1;
          console.log("- Terminal state: YES");
          console.log("- Actual terminal reward: " + actualReward);
        } else {
          console.log("- Terminal state: NO");
        }
        
        return bestMove.action;
      }

      function getBestAction(node) {
        if (!node.children || node.children.length === 0) {
          return { action: findlegalx(node.state, library), score: 0, visits: 0 };
        }
        
        var bestIndex = 0;
        var bestScore = node.children[0].utility;
        var bestVisits = node.children[0].visits;
        
        for (var i = 1; i < node.children.length; i++) {
          var newscore = node.children[i].utility;
          if (newscore > bestScore) {
            bestIndex = i;
            bestScore = newscore;
            bestVisits = node.children[i].visits;
          }
        }
        
        return {
          action: node.actions[bestIndex],
          score: bestScore,
          visits: bestVisits
        };
      }

      function makenode(state, mover, reward) {
        return {
          state: state,
          actions: [],
          children: [],
          mover: mover,
          utility: reward,
          visits: 0,
          isTerminal: findterminalp(state, library)
        };
      }

      function process(node, deadline) {
        if (Date.now() >= deadline) return false;
        
        if (node.isTerminal) {
          node.utility = findreward(role, node.state, library) * 1;
          node.visits += 1;
          return true;
        }
        
        if (node.children.length === 0) {
          expand(node);
          
          for (var i = 0; i < node.children.length && Date.now() < deadline; i++) {
            if (node.children[i].isTerminal) {
              node.children[i].utility = findreward(role, node.children[i].state, library) * 1;
            } else {
              dataindexing = true;
              var copy = definemorefacts([], node.children[i].state);
              var result = depthcharge(copy);
              dataindexing = false;
              
              node.children[i].utility = result;
            }
            node.children[i].visits = 1;
          }
          
          return update(node);
        } else {
          var child = select(node);
          process(child, deadline);
          return update(node);
        }
      }

      function expand(node) {
        var actions = findlegals(node.state, library);
        actions = shuffle(actions);
        
        for (var i = 0; i < actions.length; i++) {
          var newstate = simulate(actions[i], node.state, library);
          var newcontrol = findcontrol(newstate, library);
          var terminal = findterminalp(newstate, library);
          
          var newutility = terminal ? 
            (findreward(role, newstate, library) * 1) : 
            evaluate(role, newstate);
          
          node.actions.push(actions[i]);
          var childNode = makenode(newstate, newcontrol, newutility);
          childNode.isTerminal = terminal;
          node.children.push(childNode);
        }
        
        return true;
      }

      function select(node) {
        var total = node.visits;
        var child = node.children[0];
        var score = ucbValue(child, total);
        
        for (var i = 1; i < node.children.length; i++) {
          var newchild = node.children[i];
          var newscore = ucbValue(newchild, total);
          if (newscore > score) {
            child = newchild;
            score = newscore;
          }
        }
        
        return child;
      }

      function ucbValue(node, parentVisits) {
        if (node.visits === 0) return Infinity;
        return (node.utility / node.visits) + C * Math.sqrt(Math.log(parentVisits) / node.visits);
      }

      function update(node) {
        if (node.isTerminal) {
          node.utility = findreward(role, node.state, library) * 1;
        } else if (node.mover === role) {
          node.utility = scoremax(node);
        } else {
          node.utility = scoremin(node);
        }
        
        node.visits = node.visits + 1;
        return true;
      }

      function scoremax(node) {
        if (node.children.length === 0) return node.utility;
        
        var score = node.children[0].utility;
        for (var i = 1; i < node.children.length; i++) {
          var newscore = node.children[i].utility;
          if (newscore > score) {
            score = newscore;
          }
        }
        return score;
      }

      function scoremin(node) {
        if (node.children.length === 0) return node.utility;
        
        var score = node.children[0].utility;
        for (var i = 1; i < node.children.length; i++) {
          var newscore = node.children[i].utility;
          if (newscore < score) {
            score = newscore;
          }
        }
        return score;
      }

      function subtree(move, node) {
        if (node.children.length === 0) {
          expand(node);
        }
        
        for (var i = 0; i < node.actions.length; i++) {
          if (equalp(move, node.actions[i])) {
            return node.children[i];
          }
        }
        
        return node;
      }

      function selectaction(node) {
        if (!node.children || node.children.length === 0) {
          return findlegalx(node.state, library);
        }
        
        for (var i = 0; i < node.children.length; i++) {
          if (node.children[i].isTerminal && node.children[i].utility === 100) {
            console.log("Found winning move! " + node.actions[i]);
            return node.actions[i];
          }
        }
        
        var bestIndex = 0;
        var bestScore = node.children[0].utility / Math.max(1, node.children[0].visits);
        var minVisits = Math.max(1, node.visits / (node.children.length * 2));
        
        for (var i = 0; i < node.children.length; i++) {
          var child = node.children[i];
          var visits = Math.max(1, child.visits);
          var avgScore = child.utility / visits;
          
          if (visits >= minVisits && avgScore > bestScore) {
            bestIndex = i;
            bestScore = avgScore;
          }
        }
        
        console.log("Selected action with avg score: " + bestScore);
        return node.actions[bestIndex];
      }

      function depthcharge(state) {
        depthcharges++;
        
        if (findterminalp(state, library)) {
          return findreward(role, state, library) * 1;
        }
        
        var actions = findlegals(state, library);
        if (actions.length === 0) {
          return 0;
        }
        
        var best = Math.floor(Math.random() * actions.length);
        var newstate;
        
        if (dataindexing) {
          compexecute(actions[best], state, library);
          newstate = state;
        } else {
          newstate = simulate(actions[best], state, library);
          newstate = definemorefacts([], newstate);
        }
        
        return depthcharge(newstate);
      }




      //default functions

      function stop(move) {
        return false;
      }

      function abort() {
        return false;
      }



      function ping() {
        return "ready";
      }

      function start(r, rs, sc, pc) {
        role = r;
        rules = rs.slice(1);
        startclock = numberize(sc);
        playclock = numberize(pc);
        library = definemorerules([], rules);
        roles = findroles(library);
        state = findinits(library);
        depthcharges = 0;
        
        tree = makenode(state, findcontrol(state, library), evaluate(role, state));
        
        return "ready";
      }


//==============================================================================
// End of player code
//==============================================================================
      </script>
  </head>

  <body bgcolor='#aabbbb' onload='doinitialize()'>
    <center>
      <table width='720' cellspacing='0' cellpadding='40' bgcolor='#ffffff'>
        <tr>
          <td>

            <center>
              <table width='640' cellpadding='0'>
                <tr>
                  <td width='180' align='center' valign='center'>
                    <img width='130' src='http://gamemaster.stanford.edu/images/ggp.jpg'/>
                  </td>
                  <td align='center'>
                    <span style='font-size:18pt'>&nbsp;</span>
                    <span style='font-size:32pt'>Gamemaster</span><br/>
                  </td>
                  <td width='180' align='center' style='color:#000066;font-size:18px'>
                    <i>General<br/>Game<br/>Playing</i>
                  </td>
                </tr>
              </table>
            </center>

            <br/>
            <table width='640' cellpadding='8' cellspacing='0' bgcolor='#f4f8f8' border='1'>
              <tr height='40'>
                 <td align='center'>
                  <table style='color:#000066;font-size:18px'>
                    <tr>
                      <td>
                  Protocol: localstorage<br/>
                  Strategy: mctsWithIndexingAndDepthCharges<br/>
                  Identifier: <span id='player'>aero</span> <img src="http://gamemaster.stanford.edu/images/pencil.gif" onclick='doplayer()'/>
                      </td>
                    </tr>
                  </table>
                </td>
              </tr>
            </table>
            <br/>

            <center>
              <br/>
              <textarea id='transcript' style='font-family:courier' rows='30' cols='80' readonly></textarea>
            </center>

          </td>
        </tr>
      </table>
    </center>
  </body>
</html>