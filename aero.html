<html>
  <head>
    <title>Aero</title>
    <script type='text/javascript' src='/epilog/javascript/epilog.js'></script>
    <script type='text/javascript' src='../javascript/localstorage.js'></script>
    <script type='text/javascript' src='../reasoning/general.js'></script>
    <script type='text/javascript'>

      //==============================================================================
      // aero.js
      //==============================================================================


    /* WRITEUP:


    We start by noting that we fixed a few issues from last week's implementation of MCTS with 
    indexing and differential updates. Namely, we realized that we weren't properly differentiating
    between an immediate loss score of 0 and an estimated score of 0. The former is an end case that 
    we have to avoid if we don't want to immediately. lose. We fix this by adding a function that 
    checks for one turn losses and steers away from moves that would result in one or two turn losses.

    1. We started with the Bigswitch strategy. During the state phase, before Headstart, we sample a 
    random action to check if the game we are in is a single or multi player game. If we're in a 
    single player game, we change certain hyperparameters, like removing mobility as a factor from 
    the evaluation function (mobilityCoef = 0.0) since there is no other player to corner us.
    We also double the UCB C constant to 2.0 to further promote exploration, since there is no 
    opponent to capitalize on a "wrong" move. We also looked at using iterative deepening
    for single player games, and found that, especially on Hunter, the tree search performed
    better than iterative deepening given the time constraints, so our final solution keeps MCTS for
    both game types.

    2. We then implemented a Headstart strategy. This works by calling the play function within the 
    start function during the game setup time, as described in lecture. The player starts building out
    the MCTS tree by repeatedly calling depth charge, which gives us more time to explore the game
    tree before having to submit a first move, increasing the player's accuracy in state estimations.

    Following the teaching team's comments about last week's write up, we also include a quick analysis
    about how we expect our player to perform in different scenarios:

    We expect the player to perform well on the following kinds of games: 
      ⁃	Because of the added Headstart and Bigswitch strategies, we expect our player to perform well 
      on larger games where we are playing against other MCTS player without these improvements, since
      we will be able to perform more depthcharges through the tree.
      ⁃	We take a pessimistic approach to our player and assume that the opponent will be making the worst
      possible choice for us in each match cycle. In the cases where this holds, we expect our predictions 
      for the future outcomes to be accurate and perform well as a consequence.

    We expect the player to perform poorly on the following kinds of games: 
      ⁃	Our player strictly assumes that the opponent uses a minimax strategy throughout the game, which is
      not subject to change. For any opponents where this is incorrect, or the game itself has some incomplete 
      information component that prevents both players from assessing the game states fully, we expect the 
      player to perform sub-optimaly. This is a point we would like to focus on for future weeks. 
      ⁃	Given our player does not update its strategy based on the opponent’s strategy, any repeated play against
      an opponent that learns how we play and adapts accordingly would also have an edge on our player
      - Finally, in games with very wide, but reletively shallow game trees, with very few winning moves,
       it is possible for the MCTS to overlook these rare wins by simply not depthcharging down the correct paths. 
       In these cases, a strategy like iterative deepening coule be more appropriate.


    */


var manager = 'manager';
var player = 'aero';

var role = 'robot';
var rules = [];
var startclock = 10;
var playclock = 10;

var library = [];
var roles = [];
var state = [];

var tree;
var C = 1.0; 
var mobilityCoef = 0.2; 
var dataindexing = false; 
var depthcharges = 0; 
var isInPlay = false;
var isMultiPlayer = true;

/* shuffle(arr): 
Description: 
- Shuffles an input array in place.
Input: 
- array: The input array to be shuffled.
Returns:
- array: The shuffled array. 
*/ 
function shuffle(array) {
  for (var i = array.length - 1; i > 0; i--) {
    var j = Math.floor(Math.random() * (i + 1));
    var temp = array[i];
    array[i] = array[j];
    array[j] = temp;
  }
  return array;
}

/* numberize(str):
Description:
- Takes in an input string, and returns the integer associated
  with the numerical value of the string. 
Input:
- str: The input string, in numerals. 
Returns:
- parseInt(str): The numerical value of the input string.
*/
function numberize(str) {
  return parseInt(str);
}

/* mobility(r, state, library):
Description:
  - Determines the mobility of a given player in a state, 
    as a function of the percentage of the available moves to the player.
Input: 
  - r: Assigned role of the player. 
  - state: The current state.
  - library: The library consisting of the state graph. 
Returns:
  - If it is the player's turn, return the percentage of legal moves for the player. 
  - Otherwise (opponent's turn), return the percentage of legal moves for the player. 
*/
function mobility(r, state, library) {
  var actions = findlegals(state, library);
  var feasibles = findactions(library);

  if (r === role) {
    return (actions.length/feasibles.length) * 100;
  } else {
    return 100 - (actions.length/feasibles.length) * 100;
  }
}

/* intermediate_reward(role, state):
Description: 
  - Computes the reward of a given state, for a given player.
Input: 
  - role: Role of the current player.
  - state: Current state. 
Returns:
  - The reward of the current state. 
*/
function intermediate_reward(role, state) {
  return findreward(role, state, library) * 1;
}

/* evaluate(role, state): 
Description:
  - Weights the long-term mobility with immediate short-term gains.
  - Designed to balance the long-term vs. short-term rewards of a given decision. 
Input: 
  - role: Role of the current player. 
  - state: Current state.
Returns:
  - The weighted valuation of the current state.
*/
function evaluate(role, state) {
  return mobilityCoef * mobility(role, state, library) + (1 - mobilityCoef) * intermediate_reward(role, state);
}

function detectGameType() {
  var actions = findlegals(state, library);
  if (actions.length === 0) return;
  
  var testState = simulate(actions[0], state, library);
  var opponents = roles.filter(r => r !== role);
  
  if (opponents.length === 0 || (opponents.length === 1 && findcontrol(testState, library) === role)) {
    isMultiPlayer = false;
    C = 2.0;
    mobilityCoef = 0;
    console.log("Single-player game detected. Adjusted strategy: C=" + C + ", mobilityCoef=" + mobilityCoef);
  } else {
    isMultiPlayer = true;
    console.log("Multi-player game detected. Using default parameters.");
  }
}

/* play(move):
Description:
  - Perform one MCTS play cycle based on the given move. 
  - Continue running MCTS cycles until the time runs out. 
  - Log results every 1000 iterations (chosen by default). 
  - Find the move with best average reward. 
Input: 
  - move: An action for which an MCTS play cycle is to be performed. 
Returns:
  - false: If it is not the player's turn.
  - bestMove.action: Otherwise, action with the best average reward.  
*/
function play(move) {
  if (move !== nil) {
    tree = subtree(move, tree);
    state = tree.state;
  } else {
    tree = makenode(state, findcontrol(state, library), evaluate(role, state));
  }
  
  if (findcontrol(state, library) !== role) {
    return false;
  }

  var deadline;
  if(isInPlay == false){
    deadline = Date.now() + (startclock -1) * 1000;
    detectGameType();
  }
  else{
    deadline = Date.now() + (playclock - 2) * 1000;
  }
  var iterations = 0;
  
  while (Date.now() < deadline) {
    process(tree, deadline);
    iterations++;
    
    if (iterations % 1000 === 0) {
      var timeRemaining = Math.max(0, (deadline - Date.now()) / 1000);
      console.log("Time remaining: " + timeRemaining.toFixed(1) + "s, iterations: " + iterations);
      
      var bestAction = getBestAction(tree);
      console.log("Current best action: " + bestAction.action + 
                  ", visits: " + bestAction.visits + 
                  ", score: " + bestAction.score);
    }
  }
  if(isInPlay == false){
    isInPlay = true;
    return;
  }
  
  var bestMove = getActionAvoidingLoss(tree);
  console.log("Final decision:");
  console.log("- Action: " + bestMove.action);
  console.log("- Score: " + bestMove.score);
  console.log("- Visits: " + bestMove.visits);
  
  var newState = simulate(bestMove.action, tree.state, library);
  var terminal = findterminalp(newState, library);
  if (terminal) {
    var actualReward = findreward(role, newState, library) * 1;
    console.log("- Terminal state: YES");
    console.log("- Actual terminal reward: " + actualReward);
  } else {
    console.log("- Terminal state: NO");
  }
  
  return bestMove.action;
}

function getActionAvoidingLoss(node) {
  if (!node.children || node.children.length === 0) {
    return { action: findlegalx(node.state, library), score: 0, visits: 0 };
  }
  
  var safeMoves = [];
  
  for (var i = 0; i < node.children.length; i++) {
    var child = node.children[i];
    
    if (child.isTerminal && child.utility === 0) {
      console.log("Avoiding immediate loss: " + node.actions[i]);
      continue;
    }
    
    if (!child.isTerminal && child.mover !== role) {
      var hasOpponentWin = false;
      
      if (child.children && child.children.length > 0) {
        for (var j = 0; j < child.children.length; j++) {
          if (child.children[j].isTerminal && child.children[j].utility === 0) {
            hasOpponentWin = true;
            console.log("Avoiding 1-turn loss: " + node.actions[i]);
            break;
          }
        }
      } else {
        var opponentActions = findlegals(child.state, library);
        for (var j = 0; j < opponentActions.length; j++) {
          var opponentState = simulate(opponentActions[j], child.state, library);
          if (findterminalp(opponentState, library) && findreward(role, opponentState, library) === 0) {
            hasOpponentWin = true;
            console.log("Avoiding potential 1-turn loss: " + node.actions[i]);
            break;
          }
        }
      }
      
      if (hasOpponentWin) continue;
    }
    
    safeMoves.push({
      index: i,
      action: node.actions[i],
      score: child.utility,
      visits: child.visits
    });
  }
  
  if (safeMoves.length === 0) {
    console.log("No safe moves found. Defaulting to best available move.");
    return getBestAction(node);
  }
  
  safeMoves.sort((a, b) => (b.score / Math.max(1, b.visits)) - (a.score / Math.max(1, a.visits)));
  
  return {
    action: safeMoves[0].action,
    score: safeMoves[0].score,
    visits: safeMoves[0].visits
  };
}

function getBestAction(node) {
  if (!node.children || node.children.length === 0) {
    return { action: findlegalx(node.state, library), score: 0, visits: 0 };
  }
  
  var bestIndex = 0;
  var bestScore = node.children[0].utility;
  var bestVisits = node.children[0].visits;
  
  for (var i = 1; i < node.children.length; i++) {
    var newscore = node.children[i].utility;
    if (newscore > bestScore) {
      bestIndex = i;
      bestScore = newscore;
      bestVisits = node.children[i].visits;
    }
  }
  
  return {
    action: node.actions[bestIndex],
    score: bestScore,
    visits: bestVisits
  };
}

function makenode(state, mover, reward) {
  return {
    state: state,
    actions: [],
    children: [],
    mover: mover,
    utility: reward,
    visits: 0,
    isTerminal: findterminalp(state, library)
  };
}

function process(node, deadline) {
  if (Date.now() >= deadline) return false;
  
  if (node.isTerminal) {
    node.utility = findreward(role, node.state, library) * 1;
    node.visits += 1;
    return true;
  }
  
  if (node.children.length === 0) {
    expand(node);
    
    for (var i = 0; i < node.children.length && Date.now() < deadline; i++) {
      if (node.children[i].isTerminal) {
        node.children[i].utility = findreward(role, node.children[i].state, library) * 1;
      } else {
        dataindexing = true;
        var copy = definemorefacts([], node.children[i].state);
        var result = depthcharge(copy);
        dataindexing = false;
        
        node.children[i].utility = result;
      }
      node.children[i].visits = 1;
    }
    
    return update(node);
  } else {
    var child = select(node);
    process(child, deadline);
    return update(node);
  }
}

function expand(node) {
  var actions = findlegals(node.state, library);
  actions = shuffle(actions);
  
  for (var i = 0; i < actions.length; i++) {
    var newstate = simulate(actions[i], node.state, library);
    var newcontrol = findcontrol(newstate, library);
    var terminal = findterminalp(newstate, library);
    
    var newutility = terminal ? 
      (findreward(role, newstate, library) * 1) : 
      evaluate(role, newstate);
    
    node.actions.push(actions[i]);
    var childNode = makenode(newstate, newcontrol, newutility);
    childNode.isTerminal = terminal;
    node.children.push(childNode);
  }
  
  return true;
}

function select(node) {
  var total = node.visits;
  var child = node.children[0];
  var score = ucbValue(child, total);
  
  for (var i = 1; i < node.children.length; i++) {
    var newchild = node.children[i];
    var newscore = ucbValue(newchild, total);
    if (newscore > score) {
      child = newchild;
      score = newscore;
    }
  }
  
  return child;
}

function ucbValue(node, parentVisits) {
  if (node.visits === 0) return Infinity;
  return (node.utility / node.visits) + C * Math.sqrt(Math.log(parentVisits) / node.visits);
}

function update(node) {
  if (node.isTerminal) {
    node.utility = findreward(role, node.state, library) * 1;
  } else if (node.mover === role) {
    node.utility = scoremax(node);
  } else {
    node.utility = scoremin(node);
  }
  
  node.visits = node.visits + 1;
  return true;
}

function scoremax(node) {
  if (node.children.length === 0) return node.utility;
  
  var score = node.children[0].utility;
  for (var i = 1; i < node.children.length; i++) {
    var newscore = node.children[i].utility;
    if (newscore > score) {
      score = newscore;
    }
  }
  return score;
}

function scoremin(node) {
  if (node.children.length === 0) return node.utility;
  
  var score = node.children[0].utility;
  for (var i = 1; i < node.children.length; i++) {
    var newscore = node.children[i].utility;
    if (newscore < score) {
      score = newscore;
    }
  }
  return score;
}

function subtree(move, node) {
  if (node.children.length === 0) {
    expand(node);
  }
  
  for (var i = 0; i < node.actions.length; i++) {
    if (equalp(move, node.actions[i])) {
      return node.children[i];
    }
  }
  
  return node;
}

function selectaction(node) {
  if (!node.children || node.children.length === 0) {
    return findlegalx(node.state, library);
  }
  
  for (var i = 0; i < node.children.length; i++) {
    if (node.children[i].isTerminal && node.children[i].utility === 100) {
      console.log("Found winning move! " + node.actions[i]);
      return node.actions[i];
    }
  }
  
  var bestIndex = 0;
  var bestScore = node.children[0].utility / Math.max(1, node.children[0].visits);
  var minVisits = Math.max(1, node.visits / (node.children.length * 2));
  
  for (var i = 0; i < node.children.length; i++) {
    var child = node.children[i];
    var visits = Math.max(1, child.visits);
    var avgScore = child.utility / visits;
    
    if (visits >= minVisits && avgScore > bestScore) {
      bestIndex = i;
      bestScore = avgScore;
    }
  }
  
  console.log("Selected action with avg score: " + bestScore);
  return node.actions[bestIndex];
}

function depthcharge(state) {
  depthcharges++;
  
  if (findterminalp(state, library)) {
    return findreward(role, state, library) * 1;
  }
  
  var actions = findlegals(state, library);
  if (actions.length === 0) {
    return 0;
  }
  
  var best = Math.floor(Math.random() * actions.length);
  var newstate;
  
  if (dataindexing) {
    compexecute(actions[best], state, library);
    newstate = state;
  } else {
    newstate = simulate(actions[best], state, library);
    newstate = definemorefacts([], newstate);
  }
  
  return depthcharge(newstate);
}

function stop(move) {
  return false;
}

function abort() {
  return false;
}

function ping() {
  return "ready";
}

function start(r, rs, sc, pc) {
  role = r;
  rules = rs.slice(1);
  startclock = numberize(sc);
  playclock = numberize(pc);
  library = definemorerules([], rules);
  roles = findroles(library);
  state = findinits(library);
  depthcharges = 0;
  isInPlay = false;
  isMultiPlayer = true;
  C = 1.0;
  mobilityCoef = 0.2;
  
  tree = makenode(state, findcontrol(state, library), evaluate(role, state));

  play(nil);

  return "ready";
}


//==============================================================================
// End of player code
//==============================================================================
      </script>
  </head>

  <body bgcolor='#aabbbb' onload='doinitialize()'>
    <center>
      <table width='720' cellspacing='0' cellpadding='40' bgcolor='#ffffff'>
        <tr>
          <td>

            <center>
              <table width='640' cellpadding='0'>
                <tr>
                  <td width='180' align='center' valign='center'>
                    <img width='130' src='http://gamemaster.stanford.edu/images/ggp.jpg'/>
                  </td>
                  <td align='center'>
                    <span style='font-size:18pt'>&nbsp;</span>
                    <span style='font-size:32pt'>Gamemaster</span><br/>
                  </td>
                  <td width='180' align='center' style='color:#000066;font-size:18px'>
                    <i>General<br/>Game<br/>Playing</i>
                  </td>
                </tr>
              </table>
            </center>

            <br/>
            <table width='640' cellpadding='8' cellspacing='0' bgcolor='#f4f8f8' border='1'>
              <tr height='40'>
                 <td align='center'>
                  <table style='color:#000066;font-size:18px'>
                    <tr>
                      <td>
                  Protocol: localstorage<br/>
                  Strategy: mctsWithIndexingAndDepthCharges<br/>
                  Identifier: <span id='player'>aero</span> <img src="http://gamemaster.stanford.edu/images/pencil.gif" onclick='doplayer()'/>
                      </td>
                    </tr>
                  </table>
                </td>
              </tr>
            </table>
            <br/>

            <center>
              <br/>
              <textarea id='transcript' style='font-family:courier' rows='30' cols='80' readonly></textarea>
            </center>

          </td>
        </tr>
      </table>
    </center>
  </body>
</html>